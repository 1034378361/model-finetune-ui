#!/usr/bin/env python
"""
è§£å¯†ç®¡ç†å™¨

ç”¨äºè§£å¯†binæ–‡ä»¶å¹¶è§£æå‡ºå‚æ•°ï¼Œæ”¯æŒä¿å­˜ä¸ºCSVæ ¼å¼
"""

import json
import logging
import io
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

import pandas as pd

from .utils import ConfigManager, EnhancedLogger, performance_monitor

logger = logging.getLogger(__name__)


class DecryptionManager:
    """è§£å¯†ç®¡ç†å™¨ï¼Œç”¨äºè§£å¯†binæ–‡ä»¶å¹¶è§£æå‚æ•°ã€‚

    æ”¯æŒè§£å¯†åŠ å¯†çš„æ¨¡å‹æ–‡ä»¶ï¼Œè§£æå‡ºwã€aã€bã€Aç³»æ•°å’ŒRangeæ•°æ®ï¼Œ
    å¹¶æä¾›CSVæ ¼å¼å¯¼å‡ºåŠŸèƒ½ã€‚
    """

    def __init__(self):
        # æ ‡å‡†æ°´è´¨å‚æ•°ï¼ˆå›ºå®šï¼‰
        self.water_params = [
            "turbidity", "ss", "sd", "do", "codmn",
            "codcr", "chla", "tn", "tp", "chroma", "nh3n"
        ]
        # ç‰¹å¾ç«™ç‚¹å°†æ ¹æ®æ•°æ®åŠ¨æ€æ¨æ–­
        self.feature_stations = None

    def get_decryption_config(self) -> Dict[str, Any]:
        """è·å–è§£å¯†é…ç½®"""
        try:
            return ConfigManager.get_encryption_config()
        except Exception as e:
            logger.error(f"æ— æ³•è·å–è§£å¯†é…ç½®: {e}")
            # æä¾›é»˜è®¤é…ç½®
            return {
                "password": "default_password",
                "salt": "default_salt",
                "iv": "default_iv"
            }

    @performance_monitor("decrypt_bin_file")
    def decrypt_bin_file(self, bin_file_path: str) -> Optional[Dict[str, Any]]:
        """
        è§£å¯†binæ–‡ä»¶

        Args:
            bin_file_path: binæ–‡ä»¶è·¯å¾„

        Returns:
            è§£å¯†åçš„æ•°æ®å­—å…¸ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            EnhancedLogger.log_operation_context(
                "decrypt_bin_file",
                file_path=bin_file_path
            )

            # æ­¥éª¤1ï¼šéªŒè¯æ–‡ä»¶è·¯å¾„
            logger.info("ğŸ” æ­¥éª¤1/4: éªŒè¯æ–‡ä»¶è·¯å¾„å’Œå±æ€§...")
            validation_result = self._validate_file_path(bin_file_path)
            if not validation_result["valid"]:
                logger.error(f"âŒ æ–‡ä»¶è·¯å¾„éªŒè¯å¤±è´¥: {validation_result['error']}")
                return None

            file_size = validation_result.get("size", 0)
            logger.info(f"âœ… æ–‡ä»¶éªŒè¯é€šè¿‡: {bin_file_path} ({file_size:,} bytes)")

            # æ­¥éª¤2ï¼šè·å–è§£å¯†é…ç½®
            logger.info("ğŸ”§ æ­¥éª¤2/4: è·å–è§£å¯†é…ç½®...")
            decryption_config = self.get_decryption_config()
            logger.info("âœ… è§£å¯†é…ç½®å·²åŠ è½½")

            # æ­¥éª¤3ï¼šæ‰§è¡Œè§£å¯†
            logger.info("ğŸ”“ æ­¥éª¤3/4: æ‰§è¡ŒBINæ–‡ä»¶è§£å¯†...")
            try:
                # é¦–å…ˆå°è¯•ä½¿ç”¨å¤–éƒ¨è§£å¯†å‡½æ•°
                from autowaterqualitymodeler.utils.encryption import decrypt_file

                decrypted_result = decrypt_file(bin_file_path)
                if decrypted_result:
                    # æ£€æŸ¥è¿”å›çš„æ˜¯å­—å…¸è¿˜æ˜¯å­—ç¬¦ä¸²
                    if isinstance(decrypted_result, dict):
                        decrypted_data = decrypted_result
                        logger.info("âœ… è§£å¯†æˆåŠŸ (è¿”å›å­—å…¸æ ¼å¼)")
                    elif isinstance(decrypted_result, str):
                        import json
                        decrypted_data = json.loads(decrypted_result)
                        logger.info("âœ… è§£å¯†æˆåŠŸ (JSONå­—ç¬¦ä¸²å·²è§£æ)")
                    else:
                        logger.error(f"âŒ æœªçŸ¥çš„è§£å¯†ç»“æœç±»å‹: {type(decrypted_result)}")
                        decrypted_data = None
                else:
                    logger.error("âŒ è§£å¯†å‡½æ•°è¿”å›ç©ºç»“æœ")
                    decrypted_data = None
            except ImportError:
                # å¦‚æœå¤–éƒ¨å‡½æ•°ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–è§£å¯†
                logger.warning("âš ï¸ å¤–éƒ¨è§£å¯†å‡½æ•°ä¸å¯ç”¨ï¼Œå°è¯•ç®€åŒ–è§£å¯†")
                decrypted_data = self._simple_decrypt(bin_file_path)

            if decrypted_data:
                # æ­¥éª¤4ï¼šéªŒè¯å’Œåˆ†æè§£å¯†æ•°æ®
                logger.info("ğŸ” æ­¥éª¤4/4: éªŒè¯è§£å¯†æ•°æ®ç»“æ„...")
                validation_result = self._validate_decrypted_data(decrypted_data)
                if not validation_result["valid"]:
                    logger.error(f"âŒ è§£å¯†æ•°æ®éªŒè¯å¤±è´¥: {validation_result['error']}")
                    return None

                # æ˜¾ç¤ºè§£å¯†ç»“æœæ‘˜è¦
                model_type = decrypted_data.get("type", "æœªçŸ¥")
                feature_count = len(self.feature_stations) if self.feature_stations else 0

                logger.info("ğŸ‰ BINæ–‡ä»¶è§£å¯†å®Œæˆï¼")
                logger.info(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯: Type {model_type} ({feature_count}ç‰¹å¾Ã—{len(self.water_params)}å‚æ•°)")
                logger.info(f"ğŸ“ æ•°æ®ç»“æ„: {len([k for k, v in decrypted_data.items() if isinstance(v, list)])}ä¸ªæ•°æ®æ•°ç»„")

                return decrypted_data
            else:
                logger.error("âŒ binæ–‡ä»¶è§£å¯†å¤±è´¥")
                return None

        except Exception as e:
            logger.error(f"âŒ è§£å¯†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None

    def _simple_decrypt(self, file_path: str) -> Optional[Dict[str, Any]]:
        """ç®€åŒ–è§£å¯†æ–¹æ³•ï¼ˆå½“å¤–éƒ¨è§£å¯†å‡½æ•°ä¸å¯ç”¨æ—¶ï¼‰"""
        try:
            # å°è¯•ç›´æ¥è¯»å–JSONï¼ˆç”¨äºæµ‹è¯•ï¼‰
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            logger.info("ä½¿ç”¨ç®€åŒ–è§£å¯†æˆåŠŸ")
            return data
        except:
            logger.error("ç®€åŒ–è§£å¯†ä¹Ÿå¤±è´¥")
            return None

    @performance_monitor("parse_to_csv_format")
    def parse_to_csv_format(self, decrypted_data: Dict[str, Any]) -> Dict[str, pd.DataFrame]:
        """
        å°†è§£å¯†æ•°æ®è§£æä¸ºCSVæ ¼å¼

        Args:
            decrypted_data: è§£å¯†åçš„æ•°æ®

        Returns:
            åŒ…å«å„ç³»æ•°DataFrameçš„å­—å…¸
        """
        try:
            model_type = decrypted_data.get("type", 0)
            feature_count = len(self.feature_stations) if self.feature_stations else 0

            logger.info("ğŸ“‹ å¼€å§‹è§£ææ•°æ®ä¸ºCSVæ ¼å¼...")
            logger.info(f"ğŸ“Š æ¨¡å‹é…ç½®: Type {model_type}, {feature_count}ä¸ªç‰¹å¾ç«™ç‚¹, {len(self.water_params)}ä¸ªæ°´è´¨å‚æ•°")

            csv_data = {}

            if model_type == 0:
                logger.info("ğŸ¯ è§£æType 0æ¨¡å‹æ•°æ® (Aç³»æ•° + Rangeæ•°æ®)...")
                csv_data.update(self._parse_type_0_data(decrypted_data))
            elif model_type == 1:
                logger.info("ğŸ¯ è§£æType 1æ¨¡å‹æ•°æ® (w, a, b, Aç³»æ•° + Rangeæ•°æ®)...")
                csv_data.update(self._parse_type_1_data(decrypted_data))
            else:
                logger.error(f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
                return {}

            # æ˜¾ç¤ºè§£æç»“æœç»Ÿè®¡
            total_cells = sum(df.size for df in csv_data.values())
            total_non_zero = sum((df != 0).sum().sum() for df in csv_data.values()
                               if df.select_dtypes(include=[float, int]).size > 0)

            logger.info("âœ… CSVæ•°æ®è§£æå®Œæˆï¼")
            logger.info(f"ğŸ“„ ç”Ÿæˆæ–‡ä»¶æ•°é‡: {len(csv_data)}ä¸ª")
            logger.info(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {total_cells:,}ä¸ªæ•°æ®å•å…ƒ, {total_non_zero:,}ä¸ªéé›¶å€¼")

            # æ˜¾ç¤ºå„æ–‡ä»¶è¯¦æƒ…
            for filename, df in csv_data.items():
                non_zero_count = (df != 0).sum().sum() if df.select_dtypes(include=[float, int]).size > 0 else df.size
                logger.info(f"  ğŸ“ˆ {filename}: {df.shape[0]}Ã—{df.shape[1]} ({non_zero_count}ä¸ªéé›¶å€¼)")

            return csv_data

        except Exception as e:
            logger.error(f"âŒ è§£æCSVæ ¼å¼æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return {}

    def _parse_type_0_data(self, data: Dict[str, Any]) -> Dict[str, pd.DataFrame]:
        """è§£æType 0æ•°æ®ï¼ˆAç³»æ•° + Rangeï¼‰"""
        csv_data = {}

        try:
            # è§£æAç³»æ•°
            if "A" in data:
                a_values = data["A"]
                if len(a_values) == len(self.water_params):
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
                    if any(pd.isna(val) for val in a_values):
                        logger.warning("Aç³»æ•°ä¸­åŒ…å«NaNå€¼")

                    df_a = pd.DataFrame(
                        {"A": a_values},
                        index=self.water_params
                    )
                    csv_data["A_coefficients"] = df_a
                    logger.info(f"è§£æAç³»æ•°: {df_a.shape}")
                else:
                    logger.error(f"Aç³»æ•°é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{len(self.water_params)}, å®é™…{len(a_values)}")

            # è§£æRangeæ•°æ®
            csv_data.update(self._parse_range_data(data))

        except Exception as e:
            logger.error(f"Type 0æ•°æ®è§£æå¤±è´¥: {str(e)}")

        return csv_data

    def _parse_type_1_data(self, data: Dict[str, Any]) -> Dict[str, pd.DataFrame]:
        """è§£æType 1æ•°æ®ï¼ˆwã€aã€bã€Aç³»æ•° + Rangeï¼‰"""
        csv_data = {}

        try:
            # è§£æwç³»æ•° (ç‰¹å¾xå‚æ•°)
            if "w" in data:
                w_values = data["w"]
                expected_size = len(self.feature_stations) * len(self.water_params)
                if len(w_values) == expected_size:
                    w_matrix = self._reshape_to_matrix(w_values, len(self.feature_stations), len(self.water_params))
                    if w_matrix:  # æ£€æŸ¥é‡å¡‘æ˜¯å¦æˆåŠŸ
                        df_w = pd.DataFrame(w_matrix, index=self.feature_stations, columns=self.water_params)
                        # æ£€æŸ¥å¼‚å¸¸å€¼
                        if df_w.isna().any().any():
                            logger.warning("wç³»æ•°ä¸­åŒ…å«NaNå€¼")
                        csv_data["w_coefficients"] = df_w
                        logger.info(f"è§£æwç³»æ•°: {df_w.shape}")
                else:
                    logger.error(f"wç³»æ•°é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{expected_size}, å®é™…{len(w_values)}")

            # è§£æaç³»æ•° (ç‰¹å¾xå‚æ•°)
            if "a" in data:
                a_values = data["a"]
                expected_size = len(self.feature_stations) * len(self.water_params)
                if len(a_values) == expected_size:
                    a_matrix = self._reshape_to_matrix(a_values, len(self.feature_stations), len(self.water_params))
                    if a_matrix:
                        df_a = pd.DataFrame(a_matrix, index=self.feature_stations, columns=self.water_params)
                        if df_a.isna().any().any():
                            logger.warning("aç³»æ•°ä¸­åŒ…å«NaNå€¼")
                        csv_data["a_coefficients"] = df_a
                        logger.info(f"è§£æaç³»æ•°: {df_a.shape}")
                else:
                    logger.error(f"aç³»æ•°é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{expected_size}, å®é™…{len(a_values)}")

            # è§£æbç³»æ•° (å‚æ•°xç‰¹å¾)
            if "b" in data:
                b_values = data["b"]
                expected_size = len(self.water_params) * len(self.feature_stations)
                if len(b_values) == expected_size:
                    b_matrix = self._reshape_to_matrix(b_values, len(self.water_params), len(self.feature_stations))
                    if b_matrix:
                        df_b = pd.DataFrame(b_matrix, index=self.water_params, columns=self.feature_stations)
                        if df_b.isna().any().any():
                            logger.warning("bç³»æ•°ä¸­åŒ…å«NaNå€¼")
                        csv_data["b_coefficients"] = df_b
                        logger.info(f"è§£æbç³»æ•°: {df_b.shape}")
                else:
                    logger.error(f"bç³»æ•°é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{expected_size}, å®é™…{len(b_values)}")

            # è§£æAç³»æ•°
            if "A" in data:
                A_values = data["A"]
                if len(A_values) == len(self.water_params):
                    if any(pd.isna(val) for val in A_values):
                        logger.warning("Aç³»æ•°ä¸­åŒ…å«NaNå€¼")
                    df_A = pd.DataFrame(
                        {"A": A_values},
                        index=self.water_params
                    )
                    csv_data["A_coefficients"] = df_A
                    logger.info(f"è§£æAç³»æ•°: {df_A.shape}")
                else:
                    logger.error(f"Aç³»æ•°é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{len(self.water_params)}, å®é™…{len(A_values)}")

            # è§£æRangeæ•°æ®
            csv_data.update(self._parse_range_data(data))

        except Exception as e:
            logger.error(f"Type 1æ•°æ®è§£æå¤±è´¥: {str(e)}")

        return csv_data

    def _parse_range_data(self, data: Dict[str, Any]) -> Dict[str, pd.DataFrame]:
        """è§£æRangeæ•°æ®"""
        csv_data = {}

        try:
            if "Range" in data:
                range_values = data["Range"]
                expected_size = len(self.water_params) * 2

                if len(range_values) == expected_size:
                    # é‡æ–°ç»„ç»‡ä¸ºmin/maxåˆ—
                    range_matrix = self._reshape_to_matrix(range_values, len(self.water_params), 2)
                    if range_matrix:
                        df_range = pd.DataFrame(range_matrix, index=self.water_params, columns=["min", "max"])

                        # æ£€æŸ¥å¼‚å¸¸å€¼
                        if df_range.isna().any().any():
                            logger.warning("Rangeæ•°æ®ä¸­åŒ…å«NaNå€¼")

                        # æ£€æŸ¥min/maxå…³ç³»åˆç†æ€§
                        invalid_ranges = df_range[df_range["min"] > df_range["max"]]
                        if not invalid_ranges.empty:
                            logger.warning(f"å‘ç°{len(invalid_ranges)}ä¸ªå‚æ•°çš„min > max: {invalid_ranges.index.tolist()}")

                        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨è´Ÿå€¼èŒƒå›´ï¼ˆå¯èƒ½ä¸åˆç†ï¼‰
                        negative_ranges = df_range[(df_range["min"] < 0) | (df_range["max"] < 0)]
                        if not negative_ranges.empty:
                            logger.warning(f"å‘ç°{len(negative_ranges)}ä¸ªå‚æ•°åŒ…å«è´Ÿå€¼: {negative_ranges.index.tolist()}")

                        csv_data["range_data"] = df_range
                        logger.info(f"è§£æRangeæ•°æ®: {df_range.shape}")
                else:
                    logger.error(f"Rangeæ•°æ®é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{expected_size}, å®é™…{len(range_values)}")

        except Exception as e:
            logger.error(f"Rangeæ•°æ®è§£æå¤±è´¥: {str(e)}")

        return csv_data

    def _reshape_to_matrix(self, flat_list: list, rows: int, cols: int) -> list:
        """å°†æ‰å¹³åŒ–åˆ—è¡¨é‡æ–°ç»„ç»‡ä¸ºçŸ©é˜µ"""
        if len(flat_list) != rows * cols:
            logger.error(f"æ•°æ®é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{rows}x{cols}={rows*cols}, å®é™…{len(flat_list)}")
            return []

        matrix = []
        for i in range(rows):
            row = flat_list[i*cols:(i+1)*cols]
            matrix.append(row)
        return matrix

    def generate_csv_files(self, csv_data: Dict[str, pd.DataFrame]) -> Dict[str, bytes]:
        """
        ç”ŸæˆCSVæ–‡ä»¶çš„å­—èŠ‚å†…å®¹

        Args:
            csv_data: åŒ…å«DataFrameçš„å­—å…¸

        Returns:
            åŒ…å«CSVæ–‡ä»¶åå’Œå­—èŠ‚å†…å®¹çš„å­—å…¸
        """
        logger.info("ğŸ’¾ å¼€å§‹ç”ŸæˆCSVæ–‡ä»¶...")
        csv_files = {}
        total_size = 0

        for data_type, df in csv_data.items():
            try:
                # ç”ŸæˆCSVå­—èŠ‚æµ
                output = io.StringIO()
                df.to_csv(output, index=True, encoding='utf-8')
                csv_content = output.getvalue().encode('utf-8')

                filename = f"{data_type}.csv"
                csv_files[filename] = csv_content
                file_size = len(csv_content)
                total_size += file_size

                logger.info(f"âœ… {filename}: {file_size:,} bytes, {df.shape[0]}Ã—{df.shape[1]}")

            except Exception as e:
                logger.error(f"âŒ ç”Ÿæˆ{data_type}çš„CSVæ–‡ä»¶å¤±è´¥: {str(e)}")

        if csv_files:
            logger.info("ğŸ‰ CSVæ–‡ä»¶ç”Ÿæˆå®Œæˆï¼")
            logger.info(f"ğŸ“„ æ–‡ä»¶æ€»æ•°: {len(csv_files)}ä¸ª")
            logger.info(f"ğŸ“Š æ€»å¤§å°: {total_size:,} bytes ({total_size/1024:.1f} KB)")

        return csv_files

    def _validate_file_path(self, file_path: str) -> Dict[str, Any]:
        """éªŒè¯æ–‡ä»¶è·¯å¾„å’ŒåŸºæœ¬å±æ€§"""
        try:
            path_obj = Path(file_path)

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not path_obj.exists():
                return {"valid": False, "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}

            # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡ä»¶ï¼ˆéç›®å½•ï¼‰
            if not path_obj.is_file():
                return {"valid": False, "error": f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {file_path}"}

            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆä¸èƒ½ä¸ºç©ºï¼Œä¸èƒ½è¿‡å¤§ï¼‰
            file_size = path_obj.stat().st_size
            if file_size == 0:
                return {"valid": False, "error": "æ–‡ä»¶ä¸ºç©º"}

            # æ£€æŸ¥æ–‡ä»¶å¤§å°é™åˆ¶ï¼ˆ100MBï¼‰
            max_size = 100 * 1024 * 1024  # 100MB
            if file_size > max_size:
                return {"valid": False, "error": f"æ–‡ä»¶è¿‡å¤§ ({file_size} bytes > {max_size} bytes)"}

            # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
            allowed_extensions = {'.bin', '.json', '.txt'}  # å…è®¸çš„æ‰©å±•å
            if path_obj.suffix.lower() not in allowed_extensions:
                logger.warning(f"æ–‡ä»¶æ‰©å±•åä¸å¸¸è§: {path_obj.suffix}")

            return {"valid": True, "size": file_size}

        except Exception as e:
            return {"valid": False, "error": f"æ–‡ä»¶è·¯å¾„éªŒè¯å¼‚å¸¸: {str(e)}"}

    def _infer_feature_count(self, data: Dict[str, Any]) -> int:
        """ä»æ•°æ®ä¸­æ™ºèƒ½æ¨æ–­ç‰¹å¾æ•°é‡"""
        try:
            logger.info("ğŸ” æ™ºèƒ½åˆ†æç‰¹å¾é…ç½®...")
            param_count = len(self.water_params)

            # åˆ†æå„ç³»æ•°æ•°ç»„é•¿åº¦
            coeff_info = {}
            for key, value in data.items():
                if isinstance(value, list) and key != 'Range':
                    coeff_info[key] = len(value)

            logger.info(f"ğŸ“Š å‘ç°ç³»æ•°æ•°ç»„: {coeff_info}")

            # ä»wæˆ–aç³»æ•°æ¨æ–­ç‰¹å¾æ•°é‡
            for coeff_key in ['w', 'a']:
                if coeff_key in data and isinstance(data[coeff_key], list):
                    coeff_length = len(data[coeff_key])

                    # æ£€æŸ¥æ˜¯å¦èƒ½è¢«å‚æ•°æ•°é‡æ•´é™¤
                    if coeff_length % param_count == 0:
                        feature_count = coeff_length // param_count
                        logger.info(f"âœ… ä»{coeff_key}ç³»æ•°æ¨æ–­ç‰¹å¾æ•°é‡: {feature_count}ä¸ª")
                        logger.info(f"ğŸ“ è®¡ç®—: {coeff_length} Ã· {param_count} = {feature_count} (ç‰¹å¾Ã—å‚æ•°)")

                        # éªŒè¯å…¶ä»–ç³»æ•°æ˜¯å¦ä¸€è‡´
                        self._validate_feature_consistency(data, feature_count, param_count)
                        return feature_count

            # å¦‚æœæ— æ³•ä»w/aæ¨æ–­ï¼Œå°è¯•ä»bç³»æ•°æ¨æ–­
            if 'b' in data and isinstance(data['b'], list):
                b_length = len(data['b'])

                if b_length % param_count == 0:
                    feature_count = b_length // param_count
                    logger.info(f"âœ… ä»bç³»æ•°æ¨æ–­ç‰¹å¾æ•°é‡: {feature_count}ä¸ª")
                    logger.info(f"ğŸ“ è®¡ç®—: {b_length} Ã· {param_count} = {feature_count} (å‚æ•°Ã—ç‰¹å¾)")
                    return feature_count

            # é»˜è®¤è¿”å›26ï¼ˆå‘åå…¼å®¹ï¼‰
            logger.warning("âš ï¸ æ— æ³•æ¨æ–­ç‰¹å¾æ•°é‡ï¼Œä½¿ç”¨é»˜è®¤å€¼26")
            return 26

        except Exception as e:
            logger.error(f"âŒ æ¨æ–­ç‰¹å¾æ•°é‡æ—¶å‡ºé”™: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤å€¼26")
            return 26

    def _validate_feature_consistency(self, data: Dict[str, Any], feature_count: int, param_count: int):
        """éªŒè¯ç‰¹å¾æ•°é‡ä¸€è‡´æ€§"""
        try:
            expected_sizes = {
                'w': feature_count * param_count,
                'a': feature_count * param_count,
                'b': param_count * feature_count,
                'A': param_count,
                'Range': param_count * 2
            }

            inconsistencies = []
            for key, expected_size in expected_sizes.items():
                if key in data and isinstance(data[key], list):
                    actual_size = len(data[key])
                    if actual_size == expected_size:
                        logger.info(f"  âœ… {key}: {actual_size} (ç¬¦åˆé¢„æœŸ)")
                    else:
                        inconsistencies.append(f"{key}: {actual_size}â‰ {expected_size}")
                        logger.warning(f"  âš ï¸ {key}: {actual_size} (æœŸæœ›{expected_size})")

            if inconsistencies:
                logger.warning(f"âš ï¸ å‘ç°{len(inconsistencies)}ä¸ªç»´åº¦ä¸ä¸€è‡´: {', '.join(inconsistencies)}")
            else:
                logger.info("âœ… æ‰€æœ‰ç³»æ•°ç»´åº¦ä¸€è‡´æ€§éªŒè¯é€šè¿‡")

        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾ä¸€è‡´æ€§éªŒè¯å‡ºé”™: {str(e)}")

    def _validate_decrypted_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯è§£å¯†åçš„æ•°æ®ç»“æ„"""
        try:
            # æ£€æŸ¥åŸºæœ¬ç»“æ„
            if not isinstance(data, dict):
                return {"valid": False, "error": "è§£å¯†æ•°æ®ä¸æ˜¯å­—å…¸æ ¼å¼"}

            # æ£€æŸ¥typeå­—æ®µ
            if "type" not in data:
                return {"valid": False, "error": "ç¼ºå°‘æ¨¡å‹ç±»å‹å­—æ®µ 'type'"}

            model_type = data.get("type")
            if not isinstance(model_type, (int, float)):
                return {"valid": False, "error": f"æ¨¡å‹ç±»å‹å¿…é¡»æ˜¯æ•°å­—: {type(model_type)}"}

            model_type = int(model_type)
            if model_type not in [0, 1]:
                return {"valid": False, "error": f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}"}

            # æ™ºèƒ½æ¨æ–­ç‰¹å¾æ•°é‡å¹¶åŠ¨æ€è®¾ç½®
            feature_count = self._infer_feature_count(data)
            self.feature_stations = [f"STZ{i}" for i in range(1, feature_count + 1)]
            logger.info(f"åŠ¨æ€è®¾ç½®ç‰¹å¾ç«™ç‚¹: {len(self.feature_stations)}ä¸ª (STZ1-STZ{feature_count})")

            # æ ¹æ®æ¨¡å‹ç±»å‹éªŒè¯å¿…éœ€å­—æ®µ
            if model_type == 0:
                return self._validate_type_0_data(data)
            elif model_type == 1:
                return self._validate_type_1_data(data)

        except Exception as e:
            return {"valid": False, "error": f"æ•°æ®ç»“æ„éªŒè¯å¼‚å¸¸: {str(e)}"}

    def _validate_type_0_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯Type 0æ•°æ®ç»“æ„"""
        required_fields = ["A", "Range"]
        missing_fields = []

        for field in required_fields:
            if field not in data:
                missing_fields.append(field)

        if missing_fields:
            return {"valid": False, "error": f"Type 0æ¨¡å¼ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}"}

        # éªŒè¯Aç³»æ•°
        a_values = data["A"]
        if not isinstance(a_values, list):
            return {"valid": False, "error": f"Aç³»æ•°å¿…é¡»æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œå½“å‰ç±»å‹: {type(a_values)}"}

        if len(a_values) != len(self.water_params):
            return {"valid": False, "error": f"Aç³»æ•°é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{len(self.water_params)}, å®é™…{len(a_values)}"}

        # éªŒè¯Aç³»æ•°å€¼ç±»å‹å’ŒèŒƒå›´
        for i, val in enumerate(a_values):
            if not isinstance(val, (int, float)):
                return {"valid": False, "error": f"Aç³»æ•°[{i}]ä¸æ˜¯æ•°å­—ç±»å‹: {type(val)}"}
            if abs(val) > 1000:  # åˆç†æ€§æ£€æŸ¥
                logger.warning(f"Aç³»æ•°[{i}]å€¼è¾ƒå¤§: {val}")

        # éªŒè¯Rangeæ•°æ®
        range_values = data["Range"]
        if not isinstance(range_values, list):
            return {"valid": False, "error": f"Rangeæ•°æ®å¿…é¡»æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œå½“å‰ç±»å‹: {type(range_values)}"}

        expected_range_length = len(self.water_params) * 2
        if len(range_values) != expected_range_length:
            return {"valid": False, "error": f"Rangeæ•°æ®é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{expected_range_length}, å®é™…{len(range_values)}"}

        # éªŒè¯Rangeå€¼
        for i, val in enumerate(range_values):
            if not isinstance(val, (int, float)):
                return {"valid": False, "error": f"Rangeæ•°æ®[{i}]ä¸æ˜¯æ•°å­—ç±»å‹: {type(val)}"}

        # éªŒè¯min/maxé…å¯¹
        for i in range(0, len(range_values), 2):
            if i + 1 < len(range_values):
                min_val, max_val = range_values[i], range_values[i + 1]
                if min_val > max_val:
                    logger.warning(f"Rangeæ•°æ®ç¬¬{i//2+1}ç»„: min({min_val}) > max({max_val})")

        return {"valid": True}

    def _validate_type_1_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯Type 1æ•°æ®ç»“æ„"""
        required_fields = ["w", "a", "b", "A", "Range"]
        missing_fields = []

        for field in required_fields:
            if field not in data:
                missing_fields.append(field)

        if missing_fields:
            return {"valid": False, "error": f"Type 1æ¨¡å¼ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}"}

        # éªŒè¯å„ç³»æ•°æ•°ç»„çš„é•¿åº¦
        expected_sizes = {
            "w": len(self.feature_stations) * len(self.water_params),  # 26*11
            "a": len(self.feature_stations) * len(self.water_params),  # 26*11
            "b": len(self.water_params) * len(self.feature_stations),  # 11*26
            "A": len(self.water_params),  # 11
            "Range": len(self.water_params) * 2  # 11*2
        }

        for field, expected_size in expected_sizes.items():
            field_data = data[field]

            if not isinstance(field_data, list):
                return {"valid": False, "error": f"{field}ç³»æ•°å¿…é¡»æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œå½“å‰ç±»å‹: {type(field_data)}"}

            if len(field_data) != expected_size:
                return {"valid": False, "error": f"{field}ç³»æ•°é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{expected_size}, å®é™…{len(field_data)}"}

            # éªŒè¯æ•°å€¼ç±»å‹
            for i, val in enumerate(field_data):
                if not isinstance(val, (int, float)):
                    return {"valid": False, "error": f"{field}ç³»æ•°[{i}]ä¸æ˜¯æ•°å­—ç±»å‹: {type(val)}"}

                # åˆç†æ€§æ£€æŸ¥
                if field != "Range" and abs(val) > 1000:
                    logger.warning(f"{field}ç³»æ•°[{i}]å€¼è¾ƒå¤§: {val}")

        # éªŒè¯Rangeæ•°æ®çš„min/maxé…å¯¹
        range_values = data["Range"]
        for i in range(0, len(range_values), 2):
            if i + 1 < len(range_values):
                min_val, max_val = range_values[i], range_values[i + 1]
                if min_val > max_val:
                    logger.warning(f"Rangeæ•°æ®ç¬¬{i//2+1}ç»„: min({min_val}) > max({max_val})")

        return {"valid": True}