#!/usr/bin/env python
"""
è§£å¯†ç®¡ç†å™¨

ç”¨äºè§£å¯†binæ–‡ä»¶å¹¶è§£æå‡ºå‚æ•°ï¼Œæ”¯æŒä¿å­˜ä¸ºCSVæ ¼å¼
æ ¼å¼ï¼š[IV 16å­—èŠ‚][åŠ å¯†æ•°æ®] - å…¼å®¹C++
ç»´åº¦ä»è§£å¯†åçš„æ•°æ®è‡ªåŠ¨åæ¨
"""

import json
import logging
import io
from pathlib import Path
from typing import Any, Dict, Optional

import pandas as pd

from .utils import ConfigManager, EnhancedLogger, performance_monitor

logger = logging.getLogger(__name__)


class DecryptionManager:
    """è§£å¯†ç®¡ç†å™¨ï¼Œç”¨äºè§£å¯†binæ–‡ä»¶å¹¶è§£æå‚æ•°ã€‚

    æ”¯æŒè§£å¯†åŠ å¯†çš„æ¨¡å‹æ–‡ä»¶ï¼Œè§£æå‡ºwã€aã€bã€Aç³»æ•°å’ŒRangeæ•°æ®ï¼Œ
    å¹¶æä¾›CSVæ ¼å¼å¯¼å‡ºåŠŸèƒ½ã€‚
    """

    def __init__(self):
        # æ ‡å‡†æ°´è´¨å‚æ•°ï¼ˆå›ºå®šï¼‰
        self._default_water_params = [
            "turbidity",
            "ss",
            "sd",
            "do",
            "codmn",
            "codcr",
            "chla",
            "tn",
            "tp",
            "chroma",
            "nh3n",
        ]
        # ç‰¹å¾ç«™ç‚¹é»˜è®¤26ä¸ªï¼ˆå‘åå…¼å®¹ï¼‰ï¼Œè§£å¯†æ—¶ä¼šæ ¹æ®æ•°æ®åŠ¨æ€è°ƒæ•´
        self._default_feature_stations = [f"STZ{i}" for i in range(1, 27)]
        # ä»æ–‡ä»¶ä¸­æ£€æµ‹åˆ°çš„é…ç½®
        self._detected_config: dict[str, Any] | None = None

    @property
    def water_params(self) -> list[str]:
        """è·å–æ°´è´¨å‚æ•°åˆ—è¡¨"""
        if self._detected_config and "water_params" in self._detected_config:
            return self._detected_config["water_params"]
        return self._default_water_params

    @property
    def feature_stations(self) -> list[str] | None:
        """è·å–ç‰¹å¾ç«™ç‚¹åˆ—è¡¨"""
        if self._detected_config and "feature_stations" in self._detected_config:
            return self._detected_config["feature_stations"]
        return self._default_feature_stations

    @feature_stations.setter
    def feature_stations(self, value: list[str] | None):
        """è®¾ç½®ç‰¹å¾ç«™ç‚¹åˆ—è¡¨"""
        self._default_feature_stations = value

    def get_decryption_config(self) -> Dict[str, Any]:
        """è·å–è§£å¯†é…ç½®"""
        try:
            return ConfigManager.get_encryption_config()
        except Exception as e:
            logger.error(f"æ— æ³•è·å–è§£å¯†é…ç½®: {e}")
            return {
                "password": "default_password",
                "salt": "default_salt",
                "iv": "default_iv",
            }

    @performance_monitor("decrypt_bin_file")
    def decrypt_bin_file(self, bin_file_path: str) -> Optional[Dict[str, Any]]:
        """
        è§£å¯†binæ–‡ä»¶

        Args:
            bin_file_path: binæ–‡ä»¶è·¯å¾„

        Returns:
            è§£å¯†åçš„æ•°æ®å­—å…¸ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            EnhancedLogger.log_operation_context(
                "decrypt_bin_file", file_path=bin_file_path
            )

            # æ­¥éª¤1ï¼šéªŒè¯æ–‡ä»¶è·¯å¾„
            logger.info("ğŸ” æ­¥éª¤1/5: éªŒè¯æ–‡ä»¶è·¯å¾„å’Œå±æ€§...")
            validation_result = self._validate_file_path(bin_file_path)
            if not validation_result["valid"]:
                logger.error(f"âŒ æ–‡ä»¶è·¯å¾„éªŒè¯å¤±è´¥: {validation_result['error']}")
                return None

            file_size = validation_result.get("size", 0)
            logger.info(f"âœ… æ–‡ä»¶éªŒè¯é€šè¿‡: {bin_file_path} ({file_size:,} bytes)")

            # æ­¥éª¤2ï¼šè¯»å–æ–‡ä»¶
            logger.info("ğŸ” æ­¥éª¤2/4: è¯»å–æ–‡ä»¶...")
            with open(bin_file_path, "rb") as f:
                file_data = f.read()
            logger.info(f"ğŸ“¦ æ–‡ä»¶æ•°æ®é•¿åº¦: {len(file_data)} bytes")

            # æ£€æµ‹æ–‡ä»¶æ ¼å¼
            bin_format = self._detect_bin_format(file_data)
            logger.info(f"ğŸ“‹ æ£€æµ‹åˆ°æ–‡ä»¶æ ¼å¼: {bin_format}")

            if bin_format == "hex_reverse":
                # åå…­è¿›åˆ¶æ··æ·†æ ¼å¼
                logger.info("ğŸ”“ ä½¿ç”¨åå…­è¿›åˆ¶æ··æ·†æ–¹å¼è§£å¯†...")
                decrypted_data = self._decrypt_hex_reverse(file_data)
            else:
                # AESåŠ å¯†æ ¼å¼ï¼ˆç°æœ‰é€»è¾‘ï¼‰
                # æ­¥éª¤3ï¼šè·å–è§£å¯†é…ç½®
                logger.info("ğŸ”§ æ­¥éª¤3/4: è·å–è§£å¯†é…ç½®...")
                decryption_config = self.get_decryption_config()
                logger.info("âœ… è§£å¯†é…ç½®å·²åŠ è½½")

                # æ­¥éª¤4ï¼šæ‰§è¡Œè§£å¯†
                logger.info("ğŸ”“ æ­¥éª¤4/4: æ‰§è¡ŒBINæ–‡ä»¶è§£å¯†...")
                decrypted_data = self._decrypt_with_local_module(
                    file_data, decryption_config
                )

                if not decrypted_data:
                    # å°è¯•å¤–éƒ¨è§£å¯†å‡½æ•°
                    try:
                        from autowaterqualitymodeler.utils.encryption import (
                            decrypt_file,
                        )

                        logger.info("å°è¯•ä½¿ç”¨å¤–éƒ¨è§£å¯†å‡½æ•°...")
                        decrypted_result = decrypt_file(bin_file_path)
                        if decrypted_result:
                            if isinstance(decrypted_result, dict):
                                decrypted_data = decrypted_result
                            elif isinstance(decrypted_result, str):
                                decrypted_data = json.loads(decrypted_result)
                            logger.info("âœ… å¤–éƒ¨è§£å¯†æˆåŠŸ")
                    except ImportError:
                        logger.warning("âš ï¸ å¤–éƒ¨è§£å¯†å‡½æ•°ä¸å¯ç”¨")

            if decrypted_data:
                # ä»æ•°æ®åæ¨ç»´åº¦å¹¶è®¾ç½®é…ç½®
                self._infer_dimensions_from_data(decrypted_data)

                # éªŒè¯è§£å¯†æ•°æ®
                validation_result = self._validate_decrypted_data(decrypted_data)
                if not validation_result["valid"]:
                    logger.error(f"âŒ è§£å¯†æ•°æ®éªŒè¯å¤±è´¥: {validation_result['error']}")
                    return None

                model_type = decrypted_data.get("type", "æœªçŸ¥")
                feature_count = (
                    len(self.feature_stations) if self.feature_stations else None
                )
                logger.info("ğŸ‰ BINæ–‡ä»¶è§£å¯†å®Œæˆï¼")
                feature_label = (
                    f"{feature_count}ç‰¹å¾"
                    if feature_count is not None
                    else "ç‰¹å¾æ•°ä¸é€‚ç”¨"
                )
                logger.info(
                    f"ğŸ“Š æ¨¡å‹ä¿¡æ¯: Type {model_type} ({feature_label}Ã—{len(self.water_params)}å‚æ•°)"
                )

                return decrypted_data
            else:
                logger.error("âŒ binæ–‡ä»¶è§£å¯†å¤±è´¥")
                return None

        except Exception as e:
            logger.error(f"âŒ è§£å¯†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None

    def _decrypt_with_local_module(
        self, encrypted_data: bytes, config: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨æœ¬åœ°åŠ å¯†æ¨¡å—è§£å¯†æ•°æ®

        Args:
            encrypted_data: åŠ å¯†æ•°æ®ï¼ˆåŒ…å«IVå‰ç¼€ï¼‰
            config: è§£å¯†é…ç½®ï¼ˆpassword, salt, ivï¼‰

        Returns:
            è§£å¯†åçš„æ•°æ®å­—å…¸ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            from cryptography.hazmat.primitives import hashes, padding
            from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
            from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC

            # è·å–é…ç½®å‚æ•°
            password = config.get("password", "water_quality_analysis_key")
            salt = config.get("salt", "water_quality_salt")

            # è½¬æ¢ä¸ºå­—èŠ‚
            if isinstance(password, str):
                password = password.encode("utf-8")
            if isinstance(salt, str):
                salt = salt.encode("utf-8")

            # ä»åŠ å¯†æ•°æ®ä¸­æå–IVï¼ˆå‰16å­—èŠ‚ï¼‰
            if len(encrypted_data) < 16:
                logger.error("åŠ å¯†æ•°æ®è¿‡çŸ­ï¼Œæ— æ³•æå–IV")
                return None

            iv = encrypted_data[:16]
            ciphertext = encrypted_data[16:]

            logger.info(f"ğŸ“¦ IVé•¿åº¦: {len(iv)}, å¯†æ–‡é•¿åº¦: {len(ciphertext)}")

            # ç”Ÿæˆå¯†é’¥
            kdf = PBKDF2HMAC(
                algorithm=hashes.SHA256(),
                length=32,
                salt=salt,
                iterations=100000,
            )
            key = kdf.derive(password)

            # è§£å¯†
            cipher = Cipher(algorithms.AES(key), modes.CBC(iv))
            decryptor = cipher.decryptor()
            decrypted_padded = decryptor.update(ciphertext) + decryptor.finalize()

            # ç§»é™¤PKCS7å¡«å……
            unpadder = padding.PKCS7(128).unpadder()
            decrypted_data = unpadder.update(decrypted_padded) + unpadder.finalize()

            # è§£æJSON
            result = json.loads(decrypted_data.decode("utf-8"))
            logger.info("âœ… æœ¬åœ°è§£å¯†æˆåŠŸ")
            return result

        except Exception as e:
            logger.error(f"âŒ æœ¬åœ°è§£å¯†å¤±è´¥: {str(e)}")
            return None

    @staticmethod
    def _detect_bin_format(file_data: bytes) -> str:
        """æ£€æµ‹BINæ–‡ä»¶æ ¼å¼

        Args:
            file_data: æ–‡ä»¶åŸå§‹å­—èŠ‚æ•°æ®

        Returns:
            "hex_reverse" æˆ– "aes"
        """
        try:
            sample = file_data[:64].decode("utf-8")
            if all(c in "0123456789abcdefABCDEF" for c in sample):
                return "hex_reverse"
        except (UnicodeDecodeError, ValueError):
            pass
        return "aes"

    def _decrypt_hex_reverse(self, file_data: bytes) -> dict[str, Any] | None:
        """è§£å¯†åå…­è¿›åˆ¶å€’åºæ··æ·†æ ¼å¼çš„BINæ–‡ä»¶

        Args:
            file_data: æ–‡ä»¶åŸå§‹å­—èŠ‚æ•°æ®

        Returns:
            è§£å¯†åçš„æ•°æ®å­—å…¸ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            reversed_hex = file_data.decode("utf-8")
            hex_string = reversed_hex[::-1]
            data_json = bytes.fromhex(hex_string).decode("utf-8")
            result = json.loads(data_json)
            logger.info("âœ… åå…­è¿›åˆ¶æ··æ·†æ ¼å¼è§£å¯†æˆåŠŸ")
            return result
        except Exception as e:
            logger.error(f"âŒ åå…­è¿›åˆ¶æ··æ·†è§£å¯†å¤±è´¥: {str(e)}")
            return None

    def _simple_decrypt(self, file_path: str) -> Optional[Dict[str, Any]]:
        """ç®€åŒ–è§£å¯†æ–¹æ³•ï¼ˆå½“å¤–éƒ¨è§£å¯†å‡½æ•°ä¸å¯ç”¨æ—¶ï¼‰"""
        try:
            # å°è¯•ç›´æ¥è¯»å–JSONï¼ˆç”¨äºæµ‹è¯•ï¼‰
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            logger.info("ä½¿ç”¨ç®€åŒ–è§£å¯†æˆåŠŸ")
            return data
        except:
            logger.error("ç®€åŒ–è§£å¯†ä¹Ÿå¤±è´¥")
            return None

    @performance_monitor("parse_to_csv_format")
    def parse_to_csv_format(
        self, decrypted_data: Dict[str, Any]
    ) -> Dict[str, pd.DataFrame]:
        """
        å°†è§£å¯†æ•°æ®è§£æä¸ºCSVæ ¼å¼

        Args:
            decrypted_data: è§£å¯†åçš„æ•°æ®

        Returns:
            åŒ…å«å„ç³»æ•°DataFrameçš„å­—å…¸
        """
        try:
            model_type = decrypted_data.get("type", 0)
            feature_count = (
                len(self.feature_stations) if self.feature_stations else None
            )

            logger.info("ğŸ“‹ å¼€å§‹è§£ææ•°æ®ä¸ºCSVæ ¼å¼...")
            feature_label = (
                f"{feature_count}ä¸ªç‰¹å¾ç«™ç‚¹"
                if feature_count is not None
                else "ç‰¹å¾ç«™ç‚¹ä¸é€‚ç”¨"
            )
            logger.info(
                f"ğŸ“Š æ¨¡å‹é…ç½®: Type {model_type}, {feature_label}, {len(self.water_params)}ä¸ªæ°´è´¨å‚æ•°"
            )

            csv_data = {}

            if model_type == 0:
                logger.info("ğŸ¯ è§£æType 0æ¨¡å‹æ•°æ® (Aç³»æ•° + Rangeæ•°æ®)...")
                csv_data.update(self._parse_type_0_data(decrypted_data))
            elif model_type == 1:
                logger.info("ğŸ¯ è§£æType 1æ¨¡å‹æ•°æ® (w, a, b, Aç³»æ•° + Rangeæ•°æ®)...")
                csv_data.update(self._parse_type_1_data(decrypted_data))
            else:
                logger.error(f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
                return {}

            # æ˜¾ç¤ºè§£æç»“æœç»Ÿè®¡
            total_cells = sum(df.size for df in csv_data.values())
            total_non_zero = sum(
                (df != 0).sum().sum()
                for df in csv_data.values()
                if df.select_dtypes(include=[float, int]).size > 0
            )

            logger.info("âœ… CSVæ•°æ®è§£æå®Œæˆï¼")
            logger.info(f"ğŸ“„ ç”Ÿæˆæ–‡ä»¶æ•°é‡: {len(csv_data)}ä¸ª")
            logger.info(
                f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {total_cells:,}ä¸ªæ•°æ®å•å…ƒ, {total_non_zero:,}ä¸ªéé›¶å€¼"
            )

            # æ˜¾ç¤ºå„æ–‡ä»¶è¯¦æƒ…
            for filename, df in csv_data.items():
                non_zero_count = (
                    (df != 0).sum().sum()
                    if df.select_dtypes(include=[float, int]).size > 0
                    else df.size
                )
                logger.info(
                    f"  ğŸ“ˆ {filename}: {df.shape[0]}Ã—{df.shape[1]} ({non_zero_count}ä¸ªéé›¶å€¼)"
                )

            return csv_data

        except Exception as e:
            logger.error(f"âŒ è§£æCSVæ ¼å¼æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return {}

    def _parse_type_0_data(self, data: Dict[str, Any]) -> Dict[str, pd.DataFrame]:
        """è§£æType 0æ•°æ®ï¼ˆAç³»æ•° + Rangeï¼‰"""
        csv_data = {}

        try:
            # è§£æAç³»æ•°
            if "A" in data:
                a_values = data["A"]
                if len(a_values) == len(self.water_params):
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
                    if any(pd.isna(val) for val in a_values):
                        logger.warning("Aç³»æ•°ä¸­åŒ…å«NaNå€¼")

                    df_a = pd.DataFrame({"A": a_values}, index=self.water_params)
                    csv_data["A_coefficients"] = df_a
                    logger.info(f"è§£æAç³»æ•°: {df_a.shape}")
                else:
                    logger.error(
                        f"Aç³»æ•°é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{len(self.water_params)}, å®é™…{len(a_values)}"
                    )

            # è§£æRangeæ•°æ®
            csv_data.update(self._parse_range_data(data))

        except Exception as e:
            logger.error(f"Type 0æ•°æ®è§£æå¤±è´¥: {str(e)}")

        return csv_data

    def _parse_type_1_data(self, data: Dict[str, Any]) -> Dict[str, pd.DataFrame]:
        """è§£æType 1æ•°æ®ï¼ˆwã€aã€bã€Aç³»æ•° + Rangeï¼‰"""
        csv_data = {}

        try:
            # è§£æwç³»æ•° (ç‰¹å¾xå‚æ•°)
            if "w" in data:
                w_values = data["w"]
                expected_size = len(self.feature_stations) * len(self.water_params)
                if len(w_values) == expected_size:
                    w_matrix = self._reshape_to_matrix(
                        w_values, len(self.feature_stations), len(self.water_params)
                    )
                    if w_matrix:  # æ£€æŸ¥é‡å¡‘æ˜¯å¦æˆåŠŸ
                        df_w = pd.DataFrame(
                            w_matrix,
                            index=self.feature_stations,
                            columns=self.water_params,
                        )
                        # æ£€æŸ¥å¼‚å¸¸å€¼
                        if df_w.isna().any().any():
                            logger.warning("wç³»æ•°ä¸­åŒ…å«NaNå€¼")
                        csv_data["w_coefficients"] = df_w
                        logger.info(f"è§£æwç³»æ•°: {df_w.shape}")
                else:
                    logger.error(
                        f"wç³»æ•°é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{expected_size}, å®é™…{len(w_values)}"
                    )

            # è§£æaç³»æ•° (ç‰¹å¾xå‚æ•°)
            if "a" in data:
                a_values = data["a"]
                expected_size = len(self.feature_stations) * len(self.water_params)
                if len(a_values) == expected_size:
                    a_matrix = self._reshape_to_matrix(
                        a_values, len(self.feature_stations), len(self.water_params)
                    )
                    if a_matrix:
                        df_a = pd.DataFrame(
                            a_matrix,
                            index=self.feature_stations,
                            columns=self.water_params,
                        )
                        if df_a.isna().any().any():
                            logger.warning("aç³»æ•°ä¸­åŒ…å«NaNå€¼")
                        csv_data["a_coefficients"] = df_a
                        logger.info(f"è§£æaç³»æ•°: {df_a.shape}")
                else:
                    logger.error(
                        f"aç³»æ•°é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{expected_size}, å®é™…{len(a_values)}"
                    )

            # è§£æbç³»æ•° (å‚æ•°xç‰¹å¾)
            if "b" in data:
                b_values = data["b"]
                expected_size = len(self.water_params) * len(self.feature_stations)
                if len(b_values) == expected_size:
                    b_matrix = self._reshape_to_matrix(
                        b_values, len(self.water_params), len(self.feature_stations)
                    )
                    if b_matrix:
                        df_b = pd.DataFrame(
                            b_matrix,
                            index=self.water_params,
                            columns=self.feature_stations,
                        )
                        if df_b.isna().any().any():
                            logger.warning("bç³»æ•°ä¸­åŒ…å«NaNå€¼")
                        csv_data["b_coefficients"] = df_b
                        logger.info(f"è§£æbç³»æ•°: {df_b.shape}")
                else:
                    logger.error(
                        f"bç³»æ•°é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{expected_size}, å®é™…{len(b_values)}"
                    )

            # è§£æAç³»æ•°
            if "A" in data:
                A_values = data["A"]
                if len(A_values) == len(self.water_params):
                    if any(pd.isna(val) for val in A_values):
                        logger.warning("Aç³»æ•°ä¸­åŒ…å«NaNå€¼")
                    df_A = pd.DataFrame({"A": A_values}, index=self.water_params)
                    csv_data["A_coefficients"] = df_A
                    logger.info(f"è§£æAç³»æ•°: {df_A.shape}")
                else:
                    logger.error(
                        f"Aç³»æ•°é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{len(self.water_params)}, å®é™…{len(A_values)}"
                    )

            # è§£æRangeæ•°æ®
            csv_data.update(self._parse_range_data(data))

        except Exception as e:
            logger.error(f"Type 1æ•°æ®è§£æå¤±è´¥: {str(e)}")

        return csv_data

    def _parse_range_data(self, data: Dict[str, Any]) -> Dict[str, pd.DataFrame]:
        """è§£æRangeæ•°æ®"""
        csv_data = {}

        try:
            if "Range" in data:
                range_values = data["Range"]
                expected_size = len(self.water_params) * 2

                if len(range_values) == expected_size:
                    # é‡æ–°ç»„ç»‡ä¸ºmin/maxåˆ—
                    range_matrix = self._reshape_to_matrix(
                        range_values, len(self.water_params), 2
                    )
                    if range_matrix:
                        df_range = pd.DataFrame(
                            range_matrix,
                            index=self.water_params,
                            columns=["min", "max"],
                        )

                        # æ£€æŸ¥å¼‚å¸¸å€¼
                        if df_range.isna().any().any():
                            logger.warning("Rangeæ•°æ®ä¸­åŒ…å«NaNå€¼")

                        # æ£€æŸ¥min/maxå…³ç³»åˆç†æ€§
                        invalid_ranges = df_range[df_range["min"] > df_range["max"]]
                        if not invalid_ranges.empty:
                            logger.warning(
                                f"å‘ç°{len(invalid_ranges)}ä¸ªå‚æ•°çš„min > max: {invalid_ranges.index.tolist()}"
                            )

                        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨è´Ÿå€¼èŒƒå›´ï¼ˆå¯èƒ½ä¸åˆç†ï¼‰
                        negative_ranges = df_range[
                            (df_range["min"] < 0) | (df_range["max"] < 0)
                        ]
                        if not negative_ranges.empty:
                            logger.warning(
                                f"å‘ç°{len(negative_ranges)}ä¸ªå‚æ•°åŒ…å«è´Ÿå€¼: {negative_ranges.index.tolist()}"
                            )

                        csv_data["range_data"] = df_range
                        logger.info(f"è§£æRangeæ•°æ®: {df_range.shape}")
                else:
                    logger.error(
                        f"Rangeæ•°æ®é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{expected_size}, å®é™…{len(range_values)}"
                    )

        except Exception as e:
            logger.error(f"Rangeæ•°æ®è§£æå¤±è´¥: {str(e)}")

        return csv_data

    def _reshape_to_matrix(self, flat_list: list, rows: int, cols: int) -> list:
        """å°†æ‰å¹³åŒ–åˆ—è¡¨é‡æ–°ç»„ç»‡ä¸ºçŸ©é˜µ"""
        if len(flat_list) != rows * cols:
            logger.error(
                f"æ•°æ®é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{rows}x{cols}={rows * cols}, å®é™…{len(flat_list)}"
            )
            return []

        matrix = []
        for i in range(rows):
            row = flat_list[i * cols : (i + 1) * cols]
            matrix.append(row)
        return matrix

    def generate_csv_files(self, csv_data: Dict[str, pd.DataFrame]) -> Dict[str, bytes]:
        """
        ç”ŸæˆCSVæ–‡ä»¶çš„å­—èŠ‚å†…å®¹

        Args:
            csv_data: åŒ…å«DataFrameçš„å­—å…¸

        Returns:
            åŒ…å«CSVæ–‡ä»¶åå’Œå­—èŠ‚å†…å®¹çš„å­—å…¸
        """
        logger.info("ğŸ’¾ å¼€å§‹ç”ŸæˆCSVæ–‡ä»¶...")
        csv_files = {}
        total_size = 0

        for data_type, df in csv_data.items():
            try:
                # ç”ŸæˆCSVå­—èŠ‚æµ
                output = io.StringIO()
                df.to_csv(output, index=True, encoding="utf-8")
                csv_content = output.getvalue().encode("utf-8")

                filename = f"{data_type}.csv"
                csv_files[filename] = csv_content
                file_size = len(csv_content)
                total_size += file_size

                logger.info(
                    f"âœ… {filename}: {file_size:,} bytes, {df.shape[0]}Ã—{df.shape[1]}"
                )

            except Exception as e:
                logger.error(f"âŒ ç”Ÿæˆ{data_type}çš„CSVæ–‡ä»¶å¤±è´¥: {str(e)}")

        if csv_files:
            logger.info("ğŸ‰ CSVæ–‡ä»¶ç”Ÿæˆå®Œæˆï¼")
            logger.info(f"ğŸ“„ æ–‡ä»¶æ€»æ•°: {len(csv_files)}ä¸ª")
            logger.info(f"ğŸ“Š æ€»å¤§å°: {total_size:,} bytes ({total_size / 1024:.1f} KB)")

        return csv_files

    def _validate_file_path(self, file_path: str) -> Dict[str, Any]:
        """éªŒè¯æ–‡ä»¶è·¯å¾„å’ŒåŸºæœ¬å±æ€§"""
        try:
            path_obj = Path(file_path)

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not path_obj.exists():
                return {"valid": False, "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}

            # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡ä»¶ï¼ˆéç›®å½•ï¼‰
            if not path_obj.is_file():
                return {"valid": False, "error": f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {file_path}"}

            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆä¸èƒ½ä¸ºç©ºï¼Œä¸èƒ½è¿‡å¤§ï¼‰
            file_size = path_obj.stat().st_size
            if file_size == 0:
                return {"valid": False, "error": "æ–‡ä»¶ä¸ºç©º"}

            # æ£€æŸ¥æ–‡ä»¶å¤§å°é™åˆ¶ï¼ˆ100MBï¼‰
            max_size = 100 * 1024 * 1024  # 100MB
            if file_size > max_size:
                return {
                    "valid": False,
                    "error": f"æ–‡ä»¶è¿‡å¤§ ({file_size} bytes > {max_size} bytes)",
                }

            # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
            allowed_extensions = {".bin", ".json", ".txt"}  # å…è®¸çš„æ‰©å±•å
            if path_obj.suffix.lower() not in allowed_extensions:
                logger.warning(f"æ–‡ä»¶æ‰©å±•åä¸å¸¸è§: {path_obj.suffix}")

            return {"valid": True, "size": file_size}

        except Exception as e:
            return {"valid": False, "error": f"æ–‡ä»¶è·¯å¾„éªŒè¯å¼‚å¸¸: {str(e)}"}

    def _infer_dimensions_from_data(self, data: Dict[str, Any]) -> None:
        """ä»è§£å¯†æ•°æ®åæ¨ç»´åº¦å¹¶è®¾ç½®é…ç½®"""
        param_count, feature_count = self._infer_dimensions(data)

        # å½“feature_countä¸ºNoneæ—¶ï¼ˆType 0åœºæ™¯ï¼‰ï¼Œè®¾ç½®feature_stationsä¸ºNone
        if feature_count is None:
            if param_count != len(self._default_water_params):
                self._detected_config = {
                    "water_params": [f"param_{i + 1}" for i in range(param_count)],
                    "feature_stations": None,
                }
            else:
                self._detected_config = {
                    "water_params": self._default_water_params,
                    "feature_stations": None,
                }
            logger.info(f"ğŸ“ ç»´åº¦ç»“æœ: {param_count}å‚æ•°, ç‰¹å¾æ•°ä¸é€‚ç”¨")
            return

        # ç”Ÿæˆå‚æ•°åå’Œç«™ç‚¹å
        if param_count != len(self._default_water_params):
            self._detected_config = {
                "water_params": [f"param_{i + 1}" for i in range(param_count)],
                "feature_stations": [f"STZ{i + 1}" for i in range(feature_count)],
            }
            logger.info(f"ğŸ“ åæ¨ç»´åº¦: {param_count}å‚æ•° Ã— {feature_count}ç‰¹å¾")
        else:
            self._detected_config = {
                "water_params": self._default_water_params,
                "feature_stations": [f"STZ{i + 1}" for i in range(feature_count)],
            }
            logger.info(f"ğŸ“ ä½¿ç”¨é»˜è®¤å‚æ•°åï¼Œ{feature_count}ä¸ªç‰¹å¾ç«™ç‚¹")

    def _infer_dimensions(self, data: Dict[str, Any]) -> tuple[int, int | None]:
        """
        ä»æ•°æ®ä¸­è‡ªé€‚åº”æ¨æ–­æŒ‡æ ‡æ•°å’Œç‰¹å¾æ•°

        ç­–ç•¥ï¼š
        1. ç”¨Aå‚æ•°é•¿åº¦ç¡®å®šæŒ‡æ ‡æ•°ï¼ˆAé•¿åº¦ = æŒ‡æ ‡æ•°ï¼‰
        2. ç”¨wæˆ–aç³»æ•°é•¿åº¦é™¤ä»¥æŒ‡æ ‡æ•°å¾—åˆ°ç‰¹å¾æ•°
        3. éªŒè¯å…¶ä»–ç³»æ•°æ˜¯å¦ä¸€è‡´

        Args:
            data: è§£å¯†åçš„æ•°æ®å­—å…¸

        Returns:
            (param_count, feature_count) æŒ‡æ ‡æ•°å’Œç‰¹å¾æ•°
        """
        try:
            logger.info("=" * 50)
            logger.info("ğŸ” [ç»´åº¦æ¨æ–­] å¼€å§‹è‡ªé€‚åº”æ¨æ–­æ•°æ®ç»´åº¦...")

            # åˆ†æå„ç³»æ•°æ•°ç»„é•¿åº¦
            coeff_info = {}
            for key, value in data.items():
                if isinstance(value, list):
                    coeff_info[key] = len(value)
            logger.info(f"ğŸ“Š [ç»´åº¦æ¨æ–­] åŸå§‹æ•°æ®é•¿åº¦: {coeff_info}")

            # æ­¥éª¤1: ä»Aå‚æ•°ç¡®å®šæŒ‡æ ‡æ•°
            if "A" not in data or not isinstance(data["A"], list):
                logger.warning("âš ï¸ [ç»´åº¦æ¨æ–­] æœªæ‰¾åˆ°Aå‚æ•°ï¼Œä½¿ç”¨é»˜è®¤æŒ‡æ ‡æ•°11")
                param_count = 11
            else:
                param_count = len(data["A"])
                logger.info(
                    f"âœ… [ç»´åº¦æ¨æ–­] ä»Aå‚æ•°æ¨æ–­æŒ‡æ ‡æ•°: {param_count}ä¸ª (Aé•¿åº¦={param_count})"
                )

            # æ­¥éª¤2: ä»wæˆ–aç³»æ•°æ¨æ–­ç‰¹å¾æ•°
            feature_count = None
            for coeff_key in ["w", "a"]:
                if coeff_key in data and isinstance(data[coeff_key], list):
                    coeff_length = len(data[coeff_key])
                    logger.info(
                        f"ğŸ“ [ç»´åº¦æ¨æ–­] å°è¯•ä»{coeff_key}æ¨æ–­: é•¿åº¦={coeff_length}, æŒ‡æ ‡æ•°={param_count}"
                    )
                    if coeff_length % param_count == 0:
                        feature_count = coeff_length // param_count
                        logger.info(
                            f"âœ… [ç»´åº¦æ¨æ–­] ä»{coeff_key}ç³»æ•°æ¨æ–­ç‰¹å¾æ•°: {feature_count}ä¸ª"
                        )
                        logger.info(
                            f"ğŸ“ [ç»´åº¦æ¨æ–­] è®¡ç®—å…¬å¼: {coeff_length} Ã· {param_count} = {feature_count}"
                        )
                        break
                    else:
                        logger.warning(
                            f"âš ï¸ [ç»´åº¦æ¨æ–­] {coeff_key}ç³»æ•°é•¿åº¦{coeff_length}ä¸èƒ½è¢«æŒ‡æ ‡æ•°{param_count}æ•´é™¤"
                        )

            # æ­¥éª¤3: å¦‚æœw/aéƒ½æ²¡æœ‰ï¼Œå°è¯•ä»bæ¨æ–­
            if feature_count is None and "b" in data and isinstance(data["b"], list):
                b_length = len(data["b"])
                logger.info(
                    f"ğŸ“ [ç»´åº¦æ¨æ–­] å°è¯•ä»bæ¨æ–­: é•¿åº¦={b_length}, æŒ‡æ ‡æ•°={param_count}"
                )
                if b_length % param_count == 0:
                    feature_count = b_length // param_count
                    logger.info(f"âœ… [ç»´åº¦æ¨æ–­] ä»bç³»æ•°æ¨æ–­ç‰¹å¾æ•°: {feature_count}ä¸ª")

            # æ­¥éª¤4: å¦‚æœè¿˜æ˜¯æ— æ³•æ¨æ–­ï¼Œè¿”å›Noneï¼ˆType 0åœºæ™¯æ— ç‰¹å¾ç»´åº¦ä¿¡æ¯ï¼‰
            if feature_count is None:
                logger.info(
                    "â„¹ï¸ [ç»´åº¦æ¨æ–­] æ•°æ®ä¸­æ— ç‰¹å¾ç»´åº¦ä¿¡æ¯ï¼ˆType 0æ¨¡å‹ä»…å«Aå’ŒRangeï¼‰"
                )

            # éªŒè¯Rangeæ•°æ®ä¸€è‡´æ€§
            if "Range" in data and isinstance(data["Range"], list):
                range_length = len(data["Range"])
                expected_range = param_count * 2
                if range_length != expected_range:
                    logger.warning(
                        f"âš ï¸ [ç»´åº¦æ¨æ–­] Rangeé•¿åº¦{range_length}ä¸æœŸæœ›{expected_range}ä¸ä¸€è‡´"
                    )

            feature_label = (
                f"{feature_count}ä¸ªç‰¹å¾" if feature_count is not None else "ä¸é€‚ç”¨"
            )
            logger.info(
                f"ğŸ¯ [ç»´åº¦æ¨æ–­] æœ€ç»ˆç»“æœ: {param_count}ä¸ªæŒ‡æ ‡ Ã— {feature_label}"
            )
            logger.info("=" * 50)
            return param_count, feature_count

        except Exception as e:
            logger.error(f"âŒ [ç»´åº¦æ¨æ–­] æ¨æ–­ç»´åº¦æ—¶å‡ºé”™: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            return 11, None

    def _infer_feature_count(self, data: Dict[str, Any]) -> int:
        """ä»æ•°æ®ä¸­æ™ºèƒ½æ¨æ–­ç‰¹å¾æ•°é‡ï¼ˆå‘åå…¼å®¹æ¥å£ï¼‰"""
        _, feature_count = self._infer_dimensions(data)
        return feature_count

    def _validate_feature_consistency(
        self, data: Dict[str, Any], feature_count: int, param_count: int
    ):
        """éªŒè¯ç‰¹å¾æ•°é‡ä¸€è‡´æ€§"""
        try:
            expected_sizes = {
                "w": feature_count * param_count,
                "a": feature_count * param_count,
                "b": param_count * feature_count,
                "A": param_count,
                "Range": param_count * 2,
            }

            inconsistencies = []
            for key, expected_size in expected_sizes.items():
                if key in data and isinstance(data[key], list):
                    actual_size = len(data[key])
                    if actual_size == expected_size:
                        logger.info(f"  âœ… {key}: {actual_size} (ç¬¦åˆé¢„æœŸ)")
                    else:
                        inconsistencies.append(f"{key}: {actual_size}â‰ {expected_size}")
                        logger.warning(
                            f"  âš ï¸ {key}: {actual_size} (æœŸæœ›{expected_size})"
                        )

            if inconsistencies:
                logger.warning(
                    f"âš ï¸ å‘ç°{len(inconsistencies)}ä¸ªç»´åº¦ä¸ä¸€è‡´: {', '.join(inconsistencies)}"
                )
            else:
                logger.info("âœ… æ‰€æœ‰ç³»æ•°ç»´åº¦ä¸€è‡´æ€§éªŒè¯é€šè¿‡")

        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾ä¸€è‡´æ€§éªŒè¯å‡ºé”™: {str(e)}")

    def _validate_decrypted_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯è§£å¯†åçš„æ•°æ®ç»“æ„"""
        try:
            # æ£€æŸ¥åŸºæœ¬ç»“æ„
            if not isinstance(data, dict):
                return {"valid": False, "error": "è§£å¯†æ•°æ®ä¸æ˜¯å­—å…¸æ ¼å¼"}

            # æ£€æŸ¥typeå­—æ®µ
            if "type" not in data:
                return {"valid": False, "error": "ç¼ºå°‘æ¨¡å‹ç±»å‹å­—æ®µ 'type'"}

            model_type = data.get("type")
            if not isinstance(model_type, (int, float)):
                return {
                    "valid": False,
                    "error": f"æ¨¡å‹ç±»å‹å¿…é¡»æ˜¯æ•°å­—: {type(model_type)}",
                }

            model_type = int(model_type)
            if model_type not in [0, 1]:
                return {"valid": False, "error": f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}"}

            # è‡ªé€‚åº”æ¨æ–­æŒ‡æ ‡æ•°å’Œç‰¹å¾æ•°
            param_count, feature_count = self._infer_dimensions(data)

            # åŠ¨æ€ç”Ÿæˆæ°´è´¨å‚æ•°åï¼ˆå¦‚æœä»æ•°æ®æ¨æ–­çš„æ•°é‡ä¸é»˜è®¤ä¸åŒï¼‰
            if param_count != len(self._default_water_params):
                # ç”Ÿæˆé€šç”¨å‚æ•°å param_1, param_2, ...
                self._default_water_params = [
                    f"param_{i}" for i in range(1, param_count + 1)
                ]
                logger.info(
                    f"åŠ¨æ€ç”Ÿæˆæ°´è´¨å‚æ•°å: {param_count}ä¸ª (param_1-param_{param_count})"
                )

            # åŠ¨æ€è®¾ç½®ç‰¹å¾ç«™ç‚¹
            if feature_count is not None:
                self.feature_stations = [f"STZ{i}" for i in range(1, feature_count + 1)]
                logger.info(
                    f"åŠ¨æ€è®¾ç½®ç‰¹å¾ç«™ç‚¹: {feature_count}ä¸ª (STZ1-STZ{feature_count})"
                )
            else:
                self.feature_stations = None
                logger.info("ç‰¹å¾ç«™ç‚¹: ä¸é€‚ç”¨ï¼ˆType 0æ¨¡å‹æ— ç‰¹å¾ç»´åº¦ä¿¡æ¯ï¼‰")

            # æ ¹æ®æ¨¡å‹ç±»å‹éªŒè¯å¿…éœ€å­—æ®µï¼ˆä½¿ç”¨è‡ªé€‚åº”ç»´åº¦ï¼‰
            if model_type == 0:
                return self._validate_type_0_data_adaptive(data, param_count)
            elif model_type == 1:
                return self._validate_type_1_data_adaptive(
                    data, param_count, feature_count
                )

        except Exception as e:
            return {"valid": False, "error": f"æ•°æ®ç»“æ„éªŒè¯å¼‚å¸¸: {str(e)}"}

    def _validate_type_0_data_adaptive(
        self, data: Dict[str, Any], param_count: int
    ) -> Dict[str, Any]:
        """è‡ªé€‚åº”éªŒè¯Type 0æ•°æ®ç»“æ„"""
        required_fields = ["A", "Range"]
        missing_fields = [f for f in required_fields if f not in data]

        if missing_fields:
            return {
                "valid": False,
                "error": f"Type 0æ¨¡å¼ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}",
            }

        # éªŒè¯Aç³»æ•°ï¼ˆé•¿åº¦åº”ç­‰äºparam_countï¼‰
        a_values = data["A"]
        if not isinstance(a_values, list):
            return {
                "valid": False,
                "error": f"Aç³»æ•°å¿…é¡»æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œå½“å‰ç±»å‹: {type(a_values)}",
            }

        if len(a_values) != param_count:
            return {
                "valid": False,
                "error": f"Aç³»æ•°é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{param_count}, å®é™…{len(a_values)}",
            }

        # éªŒè¯Aç³»æ•°å€¼ç±»å‹
        for i, val in enumerate(a_values):
            if not isinstance(val, (int, float)):
                return {"valid": False, "error": f"Aç³»æ•°[{i}]ä¸æ˜¯æ•°å­—ç±»å‹: {type(val)}"}

        # éªŒè¯Rangeæ•°æ®ï¼ˆé•¿åº¦åº”ç­‰äºparam_count * 2ï¼‰
        range_values = data["Range"]
        if not isinstance(range_values, list):
            return {
                "valid": False,
                "error": f"Rangeæ•°æ®å¿…é¡»æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œå½“å‰ç±»å‹: {type(range_values)}",
            }

        expected_range_length = param_count * 2
        if len(range_values) != expected_range_length:
            return {
                "valid": False,
                "error": f"Rangeæ•°æ®é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{expected_range_length}, å®é™…{len(range_values)}",
            }

        # éªŒè¯Rangeå€¼ç±»å‹
        for i, val in enumerate(range_values):
            if not isinstance(val, (int, float)):
                return {
                    "valid": False,
                    "error": f"Rangeæ•°æ®[{i}]ä¸æ˜¯æ•°å­—ç±»å‹: {type(val)}",
                }

        logger.info(f"âœ… Type 0æ•°æ®éªŒè¯é€šè¿‡: {param_count}ä¸ªæŒ‡æ ‡")
        return {"valid": True}

    def _validate_type_1_data_adaptive(
        self, data: Dict[str, Any], param_count: int, feature_count: int
    ) -> Dict[str, Any]:
        """è‡ªé€‚åº”éªŒè¯Type 1æ•°æ®ç»“æ„"""
        required_fields = ["w", "a", "b", "A", "Range"]
        missing_fields = [f for f in required_fields if f not in data]

        if missing_fields:
            return {
                "valid": False,
                "error": f"Type 1æ¨¡å¼ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}",
            }

        # ä½¿ç”¨è‡ªé€‚åº”ç»´åº¦è®¡ç®—æœŸæœ›é•¿åº¦
        expected_sizes = {
            "w": feature_count * param_count,
            "a": feature_count * param_count,
            "b": param_count * feature_count,
            "A": param_count,
            "Range": param_count * 2,
        }

        for field, expected_size in expected_sizes.items():
            field_data = data[field]

            if not isinstance(field_data, list):
                return {
                    "valid": False,
                    "error": f"{field}ç³»æ•°å¿…é¡»æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œå½“å‰ç±»å‹: {type(field_data)}",
                }

            if len(field_data) != expected_size:
                return {
                    "valid": False,
                    "error": f"{field}ç³»æ•°é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{expected_size}, å®é™…{len(field_data)}",
                }

            # éªŒè¯æ•°å€¼ç±»å‹
            for i, val in enumerate(field_data):
                if not isinstance(val, (int, float)):
                    return {
                        "valid": False,
                        "error": f"{field}ç³»æ•°[{i}]ä¸æ˜¯æ•°å­—ç±»å‹: {type(val)}",
                    }

        logger.info(
            f"âœ… Type 1æ•°æ®éªŒè¯é€šè¿‡: {param_count}ä¸ªæŒ‡æ ‡ Ã— {feature_count}ä¸ªç‰¹å¾"
        )
        return {"valid": True}

    def _validate_type_0_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯Type 0æ•°æ®ç»“æ„ï¼ˆå‘åå…¼å®¹ï¼‰"""
        required_fields = ["A", "Range"]
        missing_fields = []

        for field in required_fields:
            if field not in data:
                missing_fields.append(field)

        if missing_fields:
            return {
                "valid": False,
                "error": f"Type 0æ¨¡å¼ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}",
            }

        # éªŒè¯Aç³»æ•°
        a_values = data["A"]
        if not isinstance(a_values, list):
            return {
                "valid": False,
                "error": f"Aç³»æ•°å¿…é¡»æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œå½“å‰ç±»å‹: {type(a_values)}",
            }

        if len(a_values) != len(self.water_params):
            return {
                "valid": False,
                "error": f"Aç³»æ•°é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{len(self.water_params)}, å®é™…{len(a_values)}",
            }

        # éªŒè¯Aç³»æ•°å€¼ç±»å‹å’ŒèŒƒå›´
        for i, val in enumerate(a_values):
            if not isinstance(val, (int, float)):
                return {"valid": False, "error": f"Aç³»æ•°[{i}]ä¸æ˜¯æ•°å­—ç±»å‹: {type(val)}"}
            if abs(val) > 1000:  # åˆç†æ€§æ£€æŸ¥
                logger.warning(f"Aç³»æ•°[{i}]å€¼è¾ƒå¤§: {val}")

        # éªŒè¯Rangeæ•°æ®
        range_values = data["Range"]
        if not isinstance(range_values, list):
            return {
                "valid": False,
                "error": f"Rangeæ•°æ®å¿…é¡»æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œå½“å‰ç±»å‹: {type(range_values)}",
            }

        expected_range_length = len(self.water_params) * 2
        if len(range_values) != expected_range_length:
            return {
                "valid": False,
                "error": f"Rangeæ•°æ®é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{expected_range_length}, å®é™…{len(range_values)}",
            }

        # éªŒè¯Rangeå€¼
        for i, val in enumerate(range_values):
            if not isinstance(val, (int, float)):
                return {
                    "valid": False,
                    "error": f"Rangeæ•°æ®[{i}]ä¸æ˜¯æ•°å­—ç±»å‹: {type(val)}",
                }

        # éªŒè¯min/maxé…å¯¹
        for i in range(0, len(range_values), 2):
            if i + 1 < len(range_values):
                min_val, max_val = range_values[i], range_values[i + 1]
                if min_val > max_val:
                    logger.warning(
                        f"Rangeæ•°æ®ç¬¬{i // 2 + 1}ç»„: min({min_val}) > max({max_val})"
                    )

        return {"valid": True}

    def _validate_type_1_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯Type 1æ•°æ®ç»“æ„"""
        required_fields = ["w", "a", "b", "A", "Range"]
        missing_fields = []

        for field in required_fields:
            if field not in data:
                missing_fields.append(field)

        if missing_fields:
            return {
                "valid": False,
                "error": f"Type 1æ¨¡å¼ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}",
            }

        # éªŒè¯å„ç³»æ•°æ•°ç»„çš„é•¿åº¦
        expected_sizes = {
            "w": len(self.feature_stations) * len(self.water_params),  # 26*11
            "a": len(self.feature_stations) * len(self.water_params),  # 26*11
            "b": len(self.water_params) * len(self.feature_stations),  # 11*26
            "A": len(self.water_params),  # 11
            "Range": len(self.water_params) * 2,  # 11*2
        }

        for field, expected_size in expected_sizes.items():
            field_data = data[field]

            if not isinstance(field_data, list):
                return {
                    "valid": False,
                    "error": f"{field}ç³»æ•°å¿…é¡»æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œå½“å‰ç±»å‹: {type(field_data)}",
                }

            if len(field_data) != expected_size:
                return {
                    "valid": False,
                    "error": f"{field}ç³»æ•°é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{expected_size}, å®é™…{len(field_data)}",
                }

            # éªŒè¯æ•°å€¼ç±»å‹
            for i, val in enumerate(field_data):
                if not isinstance(val, (int, float)):
                    return {
                        "valid": False,
                        "error": f"{field}ç³»æ•°[{i}]ä¸æ˜¯æ•°å­—ç±»å‹: {type(val)}",
                    }

                # åˆç†æ€§æ£€æŸ¥
                if field != "Range" and abs(val) > 1000:
                    logger.warning(f"{field}ç³»æ•°[{i}]å€¼è¾ƒå¤§: {val}")

        # éªŒè¯Rangeæ•°æ®çš„min/maxé…å¯¹
        range_values = data["Range"]
        for i in range(0, len(range_values), 2):
            if i + 1 < len(range_values):
                min_val, max_val = range_values[i], range_values[i + 1]
                if min_val > max_val:
                    logger.warning(
                        f"Rangeæ•°æ®ç¬¬{i // 2 + 1}ç»„: min({min_val}) > max({max_val})"
                    )

        return {"valid": True}
